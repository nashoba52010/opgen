{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "higher-packet",
   "metadata": {},
   "source": [
    "# Computation Graph\n",
    "\n",
    "This page gives an overview of the general representation of the computation graph.\n",
    "\n",
    "It must support multiple features:\n",
    "\n",
    "- static typing\n",
    "- variables (read / write / pointers)\n",
    "- 0 to many number of return values per operation\n",
    "- function calls\n",
    "\n",
    "Some other features will be added later, but design should already take them into account:\n",
    "- dynamic typing\n",
    "- control flow (loops, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-bidding",
   "metadata": {},
   "source": [
    "## Value\n",
    "\n",
    "A Value represent an actual value represented in the graph.\n",
    "It can be:\n",
    "- a placeholder / argument\n",
    "- a constant\n",
    "- the output of an operation.\n",
    "\n",
    "There is no variable type. A placeholder / const can be of type `ptr`, which can be read / written, and acts like a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floppy-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, name, ty):\n",
    "        # value name\n",
    "        self.name = name\n",
    "        \n",
    "        # graph the node belong to\n",
    "        self.graph = graph\n",
    "        \n",
    "        # value type (might be tensor, pointer, void, or others)\n",
    "        self.ty = ty\n",
    "        \n",
    "        # Operation defining self (might be none)\n",
    "        self.opdef = None\n",
    "        \n",
    "        # Operations that have self as input\n",
    "        self.users = []\n",
    "        \n",
    "class ConstValue(Value):\n",
    "    \n",
    "    def __init__(self, val):\n",
    "        # val transformed to understandable representation for the framework\n",
    "        # stored as data in the host memory\n",
    "        # will be copied to device memory only at exec time\n",
    "        pass\n",
    "    \n",
    "    # returns true if it's tensor with all values same\n",
    "    def is_splat(self): pass\n",
    "\n",
    "class PlaceholderValue(Value):\n",
    "    \n",
    "    def __init__(self, ty):\n",
    "        # Value will be given at execution time\n",
    "        # Must be of same type as ty\n",
    "        pass\n",
    "    \n",
    "class OpValue(Value):\n",
    "    \n",
    "    def __init__(self, op, output_idx):\n",
    "        # op is the operation creating this value\n",
    "        # output_idx is the index of self in the list of outputs of op\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-slope",
   "metadata": {},
   "source": [
    "# Types\n",
    "\n",
    "Every value has a type, can be any of:\n",
    "\n",
    "- Void: Not sure it's really useful yet, an op that returns nothing just returns no value, and not a value type void, but maybe could be useful for some special ops / templating.\n",
    "- StaticTensor: Tensor with known datatype and known shape\n",
    "- DynamicTensor: Tensor with known datatype and unknown shape.\n",
    "\n",
    "Dynamic tensors are needed for some special operations (eg tile with non-constant value).  \n",
    "Maybe could also be used for parameters with unknown types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-sigma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-reduction",
   "metadata": {},
   "source": [
    "## Operation\n",
    "\n",
    "An operation takes any number of values as inputs, and produce any nomber of values as outputs.  \n",
    "It also takes static arguments, knownn when building the ops. It can be things such as padding sizes for conv, or axes for permute or sum.\n",
    "They are operations for all types of operations:  \n",
    "Eg: add, mul, sub\n",
    "\n",
    "### Versions\n",
    "\n",
    "An operation may have several versions.  \n",
    "The common use case is to have one version for every type (eg 1 version for add f32, 1 version for add f64).  \n",
    "Usually the only difference between versions is the actual implementation. (all versions have it's own implementation).  \n",
    "The version is chosen automatically when the op is built, based on the inputs and attrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "local-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Op:\n",
    "    \n",
    "    def __init__(self, inputs, outputs, attrs, name):\n",
    "        # operation name\n",
    "        self.name = name\n",
    "        \n",
    "        # the graph the op belongs to\n",
    "        self.graph = graph\n",
    "        \n",
    "        # Operation version\n",
    "        self.version = self.select_version()\n",
    "        \n",
    "        #input nodes\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #output nodes for the operation (can be empty)\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        # static-time operation attributes\n",
    "        self.attrs = attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-rainbow",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "The graph represents a whole group of operations and nodes.  \n",
    "It may optionally keep tracks or not or all the ops / values created.  \n",
    "Usually the main / global graph is used to build ndoes by the user, and don't need to keep track of them (this way the nodes can be deleted as long as they are not referenced anymore).  \n",
    "Other graphs, used for optimization purposes, usually only contains nodes always usefull, and can be tracked of on the class.\n",
    "\n",
    "Despite storing nodes, graph has lots of usesful methods to:\n",
    "\n",
    "- build consts / placehodlers values\n",
    "- build operations\n",
    "- build operations to compute the gradient\n",
    "- execute nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "premier-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # If true, keeps the references to all objects in the graph, otherwhise it keeps no data\n",
    "        self.hold_refs = True\n",
    "        self.vals = dict()\n",
    "        self.ops = dict()\n",
    "        pass\n",
    "    \n",
    "    def build_placeholder(self, ty, name):\n",
    "        pass\n",
    "    \n",
    "    def build_const(self, val, name):\n",
    "        pass\n",
    "    \n",
    "    def build_op_add(self, lhs, rhs, name):\n",
    "        pass\n",
    "    \n",
    "    def build_grad_ops(self, loss_val, vals):\n",
    "        # vals is a list of value\n",
    "        # return a new list of value grads\n",
    "        # grads[i] = dloss_val/dvals[i]\n",
    "        pass\n",
    "        \n",
    "    def run(self, feeds, target_vals, target_ops):\n",
    "        # Compute all values in the list target_vals\n",
    "        # Execute all operations in the list target_ops\n",
    "        # feeds is dict<Value, nparray> contains the placeholders values need to run the ops\n",
    "        # returns list<nparray> with the computed values for target_vals\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-seeking",
   "metadata": {},
   "source": [
    "## Graph serialization\n",
    "\n",
    "Nodes of the graph can be serialized to text form (JSON).  \n",
    "This text form can be unserialized back to add nodes to a graph.  \n",
    "Every op / value is given an id, to avoid rebuilding nodes several times when serializing / unserializing again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-fitness",
   "metadata": {},
   "source": [
    "## Graph execution\n",
    "\n",
    "Here is what happens when the graph run metod is called.\n",
    "\n",
    "## Select and clone a subgraph\n",
    "\n",
    "Select and clone the subgraph of all ops that need to be executed. It uses some form of value caching, where it's not needed to recompute some values that were computed before and didn't change since last computation.  \n",
    "A value didn't change usually when none of it's predecessors was updated.  \n",
    "For example, one of the targets might have been evaluated already. So no need to recompute it.  \n",
    "Also, one intermediate value might have been a target before, so there is no need to recompute it. This OpValue is then replaced by a PlaceholderValue.  \n",
    "\n",
    "At first, can do a simple implementation that just clones the whole required subgraph, no caching.\n",
    "\n",
    "### Renaming\n",
    "\n",
    "Some ops and values need to be renamed when cloning to make sure all nodes have unique names.\n",
    "\n",
    "## Graph optimizations / compilations\n",
    "\n",
    "This pass is really backend dependant, Can do optimizations such as:\n",
    "- lowering\n",
    "- fusing\n",
    "- change to other representations.\n",
    "\n",
    "Then it can also pre-compiles the graph to be ready for execution.  \n",
    "It returns black box objects, that just takes inputs, execute the graph, and return outputs.\n",
    "\n",
    "## Graph execution\n",
    "\n",
    "The compiled graph is then used to get the results.  \n",
    "One optimization might be to cache this compiled graph, and reuse it directly, but might be complicated.  \n",
    "Maybe a feature for layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-binary",
   "metadata": {},
   "source": [
    "## Graph execution through modular implementations\n",
    "\n",
    "Every implementation contains both the methods to build the graph, and it's own compiler / executor.  \n",
    "\n",
    "This technique can be used to build the graph in one implementation / env / language, and then use another implementation to run it.  \n",
    "For example the Python API could target any implementation made on another languages.  \n",
    "Implementations can be added dynamically, without the need to recompile anything.  \n",
    "\n",
    "This is done using a middle-end C API, that allows to connect 2 implementations together. \n",
    "One implementation can be connected to make the graph run on other implementations.  \n",
    "And one implementation can be connected to allow other implementations to run this graph using this one. (can be both).  \n",
    "\n",
    "To make it possible, there must be a way:\n",
    "- to convert values from current implementation to the native general representation\n",
    "- to convert values from the native general representation to the current implementation.\n",
    "- the front implementation must call the right native functions to use other implementations\n",
    "- the back implementation must call the right naive functions to allow being used by other implementations.  \n",
    "\n",
    "The graph is serialized / unseralized to be passed between the different implementations.  \n",
    "There could be a optimization that doesn't serialize / unserialize nodes already seralized before, later feature.  \n",
    "\n",
    "This is done right when calling the run method. This way no optimization happened yet, the graph is going to be rebuild on the back implementation, and it's going to go through all the execution steps mentionned above, before returning the results to the original implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-transaction",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "These are some ideas for caching internal nodes and avoid recomputing.  \n",
    "It can only be done by the backend, I'll think more about it later.\n",
    "\n",
    "Every value is cached, to avoid creating nodes that compute the same value multiple times.\n",
    "Every value returns an hash, if a value with the same hash exist, this one is returned instead of build a new one.\n",
    "\n",
    "- Placeholders with same types / tensor shape match to the same hash, unless 2 different placeholders try to use the same one.\n",
    "- If constant is a splat, or of size < 10, it has a hash based on this. Otherwhise, all consts have a different hash.\n",
    "- Op output has an hash depending on the inputs nodes and op propperties. If inputs and properties has same, hash value is the same.\n",
    "\n",
    "### Placeholders\n",
    "\n",
    "Actually for placeholders it's a little more complicated. Sometimes, there could be more than one hit for the same node. (There could be 2 or more placeholders cached with same tensor shape/type).  \n",
    "Chosing one over the other could lead to creating lots of node, but maybe the other one had them all cached already.  \n",
    "The solution is to start matching by generate all possibles assignments, and try them them, only couting the number of cache hits. The one with the biggest number is selected, and mathcing proceeds with this.  \n",
    "\n",
    "### Consts\n",
    "\n",
    "Maybe a special thing could be done for consts too. To be able to cache all const values, no matter the shape, and use a better way to find one through the cache. Maybe store using a hash, and if there is a hit, check if it's the same tensor.  \n",
    "Or check using numpy array address ?\n",
    "\n",
    "### General hashing\n",
    "\n",
    "One solution would be to give a unique id to every value.  \n",
    "Placeholders and Consts would have their id found using the technique described earlier.\n",
    "Every op will have it's own cache, and the key is a list of integers, being hashed. These number are the inputs ids, and the attributes, converted to integers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
