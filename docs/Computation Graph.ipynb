{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "higher-packet",
   "metadata": {},
   "source": [
    "# Computation Graph\n",
    "\n",
    "This page gives an overview of the general representation of the computation graph.\n",
    "\n",
    "It must support multiple features:\n",
    "\n",
    "- static typing\n",
    "- variables (read / write / pointers)\n",
    "- 0 to many number of return values per operation\n",
    "- function calls\n",
    "\n",
    "Some other features will be added later, but design should already take them into account:\n",
    "- dynamic typing\n",
    "- control flow (loops, conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-bidding",
   "metadata": {},
   "source": [
    "## Value\n",
    "\n",
    "A Value represent an actual value represented in the graph.\n",
    "It can be:\n",
    "- a placeholder / argument\n",
    "- a constant\n",
    "- the output of an operation.\n",
    "\n",
    "There is no variable type. A placeholder / const can be of type `ptr`, which can be read / written, and acts like a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floppy-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, name, ty):\n",
    "        # value name\n",
    "        self.name = name\n",
    "        \n",
    "        # graph the node belong to\n",
    "        self.graph = graph\n",
    "        \n",
    "        # value type (might be tensor, pointer, void, or others)\n",
    "        self.ty = ty\n",
    "        \n",
    "        # Operation defining self (might be none)\n",
    "        self.opdef = None\n",
    "        \n",
    "        # Operations that have self as input\n",
    "        self.users = []\n",
    "        \n",
    "class ConstValue(Value):\n",
    "    \n",
    "    def __init__(self, val):\n",
    "        # val transformed to understandable representation for the framework\n",
    "        # stored as data in the host memory\n",
    "        # will be copied to device memory only at exec time\n",
    "        pass\n",
    "    \n",
    "    # returns true if it's tensor with all values same\n",
    "    def is_splat(self): pass\n",
    "\n",
    "class PlaceholderValue(Value):\n",
    "    \n",
    "    def __init__(self, ty):\n",
    "        # Value will be given at execution time\n",
    "        # Must be of same type as ty\n",
    "        pass\n",
    "    \n",
    "class OpValue(Value):\n",
    "    \n",
    "    def __init__(self, op, output_idx):\n",
    "        # op is the operation creating this value\n",
    "        # output_idx is the index of self in the list of outputs of op\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-slope",
   "metadata": {},
   "source": [
    "# Types\n",
    "\n",
    "Every value has a type, can be any of:\n",
    "\n",
    "- Void: Not sure it's really useful yet, an op that returns nothing just returns no value, and not a value type void, but maybe could be useful for some special ops / templating.\n",
    "- StaticTensor: Tensor with known datatype and known shape\n",
    "- DynamicTensor: Tensor with known datatype and unknown shape.\n",
    "\n",
    "Dynamic tensors are needed for some special operations (eg tile with non-constant value).  \n",
    "Maybe could also be used for parameters with unknown types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-sigma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-reduction",
   "metadata": {},
   "source": [
    "## Operation\n",
    "\n",
    "An operation takes any number of values as inputs, and produce any nomber of values as outputs.  \n",
    "It also takes static arguments, knownn when building the ops. It can be things such as padding sizes for conv, or axes for permute or sum.\n",
    "They are operations for all types of operations:  \n",
    "Eg: add, mul, sub\n",
    "\n",
    "### Versions\n",
    "\n",
    "An operation may have several versions.  \n",
    "The common use case is to have one version for every type (eg 1 version for add f32, 1 version for add f64).  \n",
    "Usually the only difference between versions is the actual implementation. (all versions have it's own implementation).  \n",
    "The version is chosen automatically when the op is built, based on the inputs and attrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "local-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Op:\n",
    "    \n",
    "    def __init__(self, inputs, outputs, attrs, name):\n",
    "        # operation name\n",
    "        self.name = name\n",
    "        \n",
    "        # Operation version\n",
    "        self.version = self.select_version()\n",
    "        \n",
    "        #input nodes\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #output nodes for the operation (can be empty)\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        # static-time operation attributes\n",
    "        self.attrs = attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-rainbow",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "The graph could represent the whole group of operations and nodes.  \n",
    "Given that the only form of caching is using the nodes itself, there is little need to have a graph class.  \n",
    "\n",
    "There is still a Graph class, but it doesn't store any data, it's only used for its methods:\n",
    "\n",
    "- build consts / placehodlers values\n",
    "- build operations\n",
    "- build operations to compute the gradient\n",
    "- execute nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premier-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # contains no data\n",
    "        pass\n",
    "    \n",
    "    def build_placeholder(self, ty, name):\n",
    "        pass\n",
    "    \n",
    "    def build_const(self, val, name):\n",
    "        pass\n",
    "    \n",
    "    def build_op_add(self, lhs, rhs, name):\n",
    "        pass\n",
    "    \n",
    "    def build_grad_ops(self, loss_val, vals):\n",
    "        # vals is a list of value\n",
    "        # return a new list of value grads\n",
    "        # grads[i] = dloss_val/dvals[i]\n",
    "        pass\n",
    "        \n",
    "    def run(self, feeds, target_vals, target_ops):\n",
    "        # Compute all values in the list target_vals\n",
    "        # Execute all operations in the list target_ops\n",
    "        # feeds is dict<Value, nparray> contains the placeholders values need to run the ops\n",
    "        # returns list<nparray> with the computed values for target_vals\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-fitness",
   "metadata": {},
   "source": [
    "## Graph execution\n",
    "\n",
    "Here is what happens when the graph run metod is called.\n",
    "\n",
    "## Select and clone a subgraph\n",
    "\n",
    "Select and clone the subgraph of all ops that need to be executed. It uses some form of value caching, where it's not needed to recompute some values that were computed before and didn't change since last computation.  \n",
    "A value didn't change usually when none of it's predecessors was updated.  \n",
    "For example, one of the targets might have been evaluated already. So no need to recompute it.  \n",
    "Also, one intermediate value might have been a target before, so there is no need to recompute it. This OpValue is then replaced by a PlaceholderValue.  \n",
    "\n",
    "At first, can do a simple implementation that just clones the whole graph.\n",
    "\n",
    "### Renaming\n",
    "\n",
    "Some ops and values need to be renamed when cloning to make sure all nodes have unique names.\n",
    "\n",
    "## Graph execution\n",
    "\n",
    "The way the graph is going to be executed really depends on the actual used implementation.  \n",
    "I can't give details about it.\n",
    "\n",
    "\n",
    "## Graph Serialization\n",
    "\n",
    "The graph is converted to text-representation. It's similar to an mlir dialect, but as JSON.  \n",
    "This JSON string and the placeholders data is given to the backend, which is responsible for computing it, and returns a list of value.\n",
    "\n",
    "## Backends\n",
    "\n",
    "The backend can be a Python Backend, or a native Backend.  \n",
    "For native backends, this is done using a C API. The idea is to create an \"Adapter\", which is a C Python module, that can load backends dynamically. This way there is no need to recompile the whole python packaage every time.  \n",
    "More infos on backends in the backends document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-transaction",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "These are some ideas for caching internal nodes and avoid recomputing.  \n",
    "It can only be done by the backend, I'll think more about it later.\n",
    "\n",
    "Every value is cached, to avoid creating nodes that compute the same value multiple times.\n",
    "Every value returns an hash, if a value with the same hash exist, this one is returned instead of build a new one.\n",
    "\n",
    "- Placeholders with same types / tensor shape match to the same hash, unless 2 different placeholders try to use the same one.\n",
    "- If constant is a splat, or of size < 10, it has a hash based on this. Otherwhise, all consts have a different hash.\n",
    "- Op output has an hash depending on the inputs nodes and op propperties. If inputs and properties has same, hash value is the same.\n",
    "\n",
    "### Placeholders\n",
    "\n",
    "Actually for placeholders it's a little more complicated. Sometimes, there could be more than one hit for the same node. (There could be 2 or more placeholders cached with same tensor shape/type).  \n",
    "Chosing one over the other could lead to creating lots of node, but maybe the other one had them all cached already.  \n",
    "The solution is to start matching by generate all possibles assignments, and try them them, only couting the number of cache hits. The one with the biggest number is selected, and mathcing proceeds with this.  \n",
    "\n",
    "### Consts\n",
    "\n",
    "Maybe a special thing could be done for consts too. To be able to cache all const values, no matter the shape, and use a better way to find one through the cache. Maybe store using a hash, and if there is a hit, check if it's the same tensor.  \n",
    "Or check using numpy array address ?\n",
    "\n",
    "### General hashing\n",
    "\n",
    "One solution would be to give a unique id to every value.  \n",
    "Placeholders and Consts would have their id found using the technique described earlier.\n",
    "Every op will have it's own cache, and the key is a list of integers, being hashed. These number are the inputs ids, and the attributes, converted to integers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
