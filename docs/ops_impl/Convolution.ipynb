{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "discrete-afternoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "def arr_diff(x, y):\n",
    "    assert list(x.shape) == list(y.shape)\n",
    "    x = np.ravel(x)\n",
    "    y = np.ravel(y)\n",
    "    return (x - y) @ (x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-fence",
   "metadata": {},
   "source": [
    "# Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "concerned-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 561 : i32}} {\n",
      "  func @__inference_tf_conv_7(%arg0: tensor<3x32x29x7xf32> {tf._user_specified_name = \"x\"}, %arg1: tensor<5x5x7x10xf32> {tf._user_specified_name = \"w\"}) -> tensor<3x32x29x10xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"x,w\", outputs = \"identity_RetVal\"}} {\n",
      "    %0 = \"tf.Conv2D\"(%arg0, %arg1) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<3x32x29x7xf32>, tensor<5x5x7x10xf32>) -> tensor<3x32x29x10xf32>\n",
      "    %1 = \"tf.Identity\"(%0) {device = \"\"} : (tensor<3x32x29x10xf32>) -> tensor<3x32x29x10xf32>\n",
      "    return %1 : tensor<3x32x29x10xf32>\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 32, 29, 7)\n",
    "w = np.random.randn(5, 5, 7, 10)\n",
    "\n",
    "@tf.function\n",
    "def tf_conv(x, w):\n",
    "    y = tf.nn.conv2d(input=x, filters=w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return y\n",
    "\n",
    "fun = tf_conv.get_concrete_function(\n",
    "    tf.TensorSpec(x.shape, tf.dtypes.float32),\n",
    "    tf.TensorSpec(w.shape, tf.dtypes.float32),\n",
    ")\n",
    "mlir = tf.mlir.experimental.convert_function(fun)\n",
    "print(mlir)\n",
    "\n",
    "#y = tf_conv(x, w)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suited-hydrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paddings: [[0, 0], [2, 2], [2, 2], [0, 0]]\n",
      "Output shape: [3, 32, 29, 10]\n",
      "4.535802833650465e-25\n"
     ]
    }
   ],
   "source": [
    "class MyConv2DDescriptor:\n",
    "    \n",
    "    def get_explicit_padding(self):\n",
    "        padding = self.padding\n",
    "        pad_top, pad_bottom, pad_left, pad_right = (None, None, None, None)\n",
    "        xHeight = self.x[1]\n",
    "        xWidth = self.x[2]\n",
    "        kHeight = self.w[0]\n",
    "        kWidth = self.w[1]\n",
    "        sh = self.strides[1]\n",
    "        sw = self.strides[2]\n",
    "        \n",
    "        if not isinstance(padding, str):\n",
    "            #explicit padding\n",
    "            #[[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]\n",
    "            assert len(padding) == 4\n",
    "            for x in padding: assert len(x) == 2\n",
    "            for x in padding[0]: assert x == 0\n",
    "            for x in padding[3]: assert x == 0\n",
    "            pad_top = padding[1][0]\n",
    "            pad_bottom = padding[1][1]\n",
    "            pad_left = padding[2][0]\n",
    "            pad_right = padding[2][1]\n",
    "        \n",
    "        elif padding == 'SAME':\n",
    "            if xHeight % sh == 0:\n",
    "                pad_height = max(kHeight - sh, 0)\n",
    "            else:\n",
    "                pad_height = max(kHeight - (xHeight % sh), 0)\n",
    "            if xWidth % sw == 0:\n",
    "                pad_width = max(kWidth - sw, 0)\n",
    "            else:\n",
    "                pad_width = max(kWidth - (xWidth % sw), 0)\n",
    "            \n",
    "            pad_top = pad_height // 2\n",
    "            pad_bottom = pad_height - pad_top\n",
    "            pad_left = pad_width // 2\n",
    "            pad_right = pad_width - pad_left\n",
    "            \n",
    "        \n",
    "        elif padding == 'VALID':\n",
    "            pad_top = 0\n",
    "            pad_bottom = 0\n",
    "            pad_left = 0\n",
    "            pad_right = 0\n",
    "            \n",
    "        else:\n",
    "            raise Exception('Unknown padding')\n",
    "        \n",
    "        self.padding = [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]\n",
    "            \n",
    "    def get_output_shape(self):\n",
    "        pad_top, pad_bottom = self.padding[1]\n",
    "        pad_left, pad_right = self.padding[2]\n",
    "        xHeight = self.x[1]\n",
    "        xWidth = self.x[2]\n",
    "        kHeight = self.w[0]\n",
    "        kWidth = self.w[1]\n",
    "        sh = self.strides[1]\n",
    "        sw = self.strides[2]\n",
    "        \n",
    "        assert self.x[3] == self.w[2]\n",
    "        \n",
    "        # Padded input dimensions\n",
    "        xPHeight = xHeight + pad_top + pad_bottom\n",
    "        xPWidth = xWidth + pad_left + pad_right\n",
    "        \n",
    "        outN = self.x[0]\n",
    "        outHeight = (xPHeight - kHeight) // sh + 1\n",
    "        outWidth = (xPWidth - kWidth) // sw + 1\n",
    "        outC = self.w[3]\n",
    "        \n",
    "        self.output_shape = [outN, outHeight, outWidth, outC]\n",
    "    \n",
    "    def __init__(self, x, w, strides, padding):\n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.output_shape = None\n",
    "                        \n",
    "        # Check strides\n",
    "        assert len(strides) == 4\n",
    "        assert strides[0] == 1\n",
    "        assert strides[3] == 1\n",
    "        \n",
    "        # Compute real paddings\n",
    "        self.get_explicit_padding()\n",
    "        print('Paddings:', self.padding)\n",
    "        \n",
    "        # Compute output shape\n",
    "        self.get_output_shape()\n",
    "        print('Output shape:', self.output_shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # compute using tf version directly\n",
    "    def compute_tf(self, x, w):\n",
    "        y = tf.nn.conv2d(input=x, filters=w, strides=self.strides, padding=[[0,0],[0,0],[0,0],[0,0]])\n",
    "        return y.numpy()\n",
    "\n",
    "    # pad input tensor using paddings infos\n",
    "    def pad_input(self, x):\n",
    "        return np.pad(x, self.padding)\n",
    "    \n",
    "    # 7 for loop using conv2d formula\n",
    "    def compute_naive(self, x, w):\n",
    "        sh = self.strides[1]\n",
    "        sw = self.strides[2]\n",
    "        kHeight = self.w[0]\n",
    "        kWidth = self.w[1]\n",
    "        xChannels = self.w[2]\n",
    "        res = np.zeros(self.output_shape)\n",
    "        \n",
    "        for b in range(self.output_shape[0]):\n",
    "            for i in range(self.output_shape[1]):\n",
    "                for j in range(self.output_shape[2]):\n",
    "                    for k in range(self.output_shape[3]):\n",
    "                        val = 0\n",
    "                        for di in range(kHeight):\n",
    "                            for dj in range(kWidth):\n",
    "                                for q in range(xChannels):\n",
    "                                    val += x[b, sh*i + di, sw * j + dj, q] * w[di, dj, q, k]\n",
    "                        res[b, i, j, k] = val\n",
    "                                \n",
    "                                \n",
    "                        \n",
    "        return res\n",
    "    \n",
    "    # flatten and extend the input to run conv2d as a matmul\n",
    "    # turned from (xB, xH, xW, xC) into (xB*yH*yW, kH*kW*xC)\n",
    "    def matmul_flatten_x(self, x):\n",
    "        sh, sw = self.strides[1:-1]\n",
    "        xB, xHeight, xWidth, xC = self.x\n",
    "        kHeight, kWidth = self.w[:2]\n",
    "        yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # divide 1 DIM into 3 to build the mat more easily\n",
    "        new_x = np.zeros((xB, yHeight, yWidth, kHeight*kWidth*xC))\n",
    "        \n",
    "        for b in range(xB):\n",
    "            for i in range(yHeight):\n",
    "                for j in range(yWidth):\n",
    "                    # extract x slice used in the sum of naive implem to compute y[b, i, j, :]\n",
    "                    new_x[b, i, j] = x[b, sh*i:sh*i+kHeight, sw*j:sw*j+kWidth, :].ravel()\n",
    "            \n",
    "        return new_x.reshape(xB * yHeight * yWidth, kHeight * kWidth * xC)\n",
    "    \n",
    "    # Compute by turning the conv into a matmul op\n",
    "    def compute_matmul(self, x, w):\n",
    "        sh, sw = self.strides[1:-1]\n",
    "        xB, xHeight, xWidth, xC = self.x\n",
    "        kHeight, kWidth = self.w[:2]\n",
    "        yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # (xB, xH, xW, xC) -> (xB*yH*yW, kH*kW*xC)\n",
    "        x = self.matmul_flatten_x(x)\n",
    "        \n",
    "        # (kH, kW, xC, yC) -> (kH*kW*xC, yC)\n",
    "        w = w.reshape(kHeight * kWidth * xC, yC)\n",
    "        \n",
    "        # [(xB*yH*yW, kH*kW*xC), (kH*kW*xC, yC)] -> (xB*yH*yW, yC)\n",
    "        y = np.matmul(x, w)\n",
    "        \n",
    "        # (xB*yH*yW, yC) -> (xB, yH, yW, yC)\n",
    "        y = y.reshape(xB, yHeight, yWidth, yC)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def compute(self, x, w):\n",
    "        assert list(x.shape) == list(self.x)\n",
    "        assert list(w.shape) == list(self.w)\n",
    "        \n",
    "        x = self.pad_input(x)\n",
    "        \n",
    "        #y = self.compute_tf(x, w)\n",
    "        #y = self.compute_naive(x, w)\n",
    "        y = self.compute_matmul(x, w)\n",
    "        \n",
    "        assert list(y.shape) == list(self.output_shape)\n",
    "        return y\n",
    "        \n",
    "        \n",
    "def my_conv2d(x, w, strides, padding):\n",
    "    op = MyConv2DDescriptor(x.shape, w.shape, strides, padding)\n",
    "    return op.compute(x, w)\n",
    "\n",
    "x = np.random.randn(3, 32, 29, 7)\n",
    "w = np.random.randn(5, 5, 7, 10)\n",
    "\n",
    "y_ref = tf_conv(x, w).numpy()\n",
    "y = my_conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "print(arr_diff(y_ref, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-natural",
   "metadata": {},
   "source": [
    "# Conv2D backprop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "wicked-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 561 : i32}} {\n",
      "  func @__inference_tf_conv_dx_575(%arg0: tensor<3x32x29x7xf32> {tf._user_specified_name = \"x\"}, %arg1: tensor<5x5x7x10xf32> {tf._user_specified_name = \"w\"}, %arg2: tensor<3x32x29x10xf32> {tf._user_specified_name = \"dy\"}) -> tensor<3x32x29x7xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"x,w,dy\", outputs = \"identity_RetVal\"}} {\n",
      "    %0 = \"tf.Const\"() {value = dense<[3, 32, 29, 7]> : tensor<4xi32>} : () -> tensor<4xi32>\n",
      "    %1 = \"tf.Conv2DBackpropInput\"(%0, %arg1, %arg2) {_class = [\"loc:@Conv2D\"], data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<4xi32>, tensor<5x5x7x10xf32>, tensor<3x32x29x10xf32>) -> tensor<3x32x29x7xf32>\n",
      "    %2 = \"tf.Identity\"(%1) {device = \"\"} : (tensor<3x32x29x7xf32>) -> tensor<3x32x29x7xf32>\n",
      "    return %2 : tensor<3x32x29x7xf32>\n",
      "  }\n",
      "}\n",
      "(3, 32, 29, 7) (5, 5, 7, 10) (3, 32, 29, 10)\n",
      "(3, 32, 29, 7)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(np.random.randn(3, 32, 29, 7).astype(np.float32))\n",
    "w = tf.constant(np.random.randn(5, 5, 7, 10).astype(np.float32))\n",
    "yshape = tf_conv(x, w).shape\n",
    "dy = tf.constant(np.random.randn(*yshape).astype(np.float32))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def tf_conv_dx(x, w, dy):\n",
    "    y = tf.nn.conv2d(input=x, filters=w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    loss = tf.reduce_sum(y*dy)\n",
    "    return tf.gradients(loss, [x], stop_gradients=[x])[0]\n",
    "\n",
    "\n",
    "fun = tf_conv_dx.get_concrete_function(\n",
    "    tf.TensorSpec(x.shape, tf.dtypes.float32),\n",
    "    tf.TensorSpec(w.shape, tf.dtypes.float32),\n",
    "    tf.TensorSpec(dy.shape, tf.dtypes.float32),\n",
    ")\n",
    "mlir = tf.mlir.experimental.convert_function(fun)\n",
    "print(mlir)\n",
    "\n",
    "dx = tf_conv_dx(x, w, dy)\n",
    "print(x.shape, w.shape, dy.shape)\n",
    "print(dx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-breath",
   "metadata": {},
   "source": [
    "## Raw call\n",
    "\n",
    "can call the raw function directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "numerous-artwork",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 561 : i32}} {\n",
      "  func @__inference_tf_conv_backprop_input_689(%arg0: tensor<5x5x7x10xf32> {tf._user_specified_name = \"w\"}, %arg1: tensor<3x32x29x10xf32> {tf._user_specified_name = \"dy\"}) -> tensor<3x32x29x7xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"w,dy\", outputs = \"identity_RetVal\"}} {\n",
      "    %0 = \"tf.Const\"() {value = dense<[3, 32, 29, 7]> : tensor<4xi32>} : () -> tensor<4xi32>\n",
      "    %1 = \"tf.Conv2DBackpropInput\"(%0, %arg0, %arg1) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<4xi32>, tensor<5x5x7x10xf32>, tensor<3x32x29x10xf32>) -> tensor<3x32x29x7xf32>\n",
      "    %2 = \"tf.Identity\"(%1) {device = \"\"} : (tensor<3x32x29x7xf32>) -> tensor<3x32x29x7xf32>\n",
      "    return %2 : tensor<3x32x29x7xf32>\n",
      "  }\n",
      "}\n",
      "(3, 32, 29, 7) (5, 5, 7, 10) (3, 32, 29, 10)\n",
      "(3, 32, 29, 7)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(np.random.randn(3, 32, 29, 7).astype(np.float32))\n",
    "w = tf.constant(np.random.randn(5, 5, 7, 10).astype(np.float32))\n",
    "yshape = tf_conv(x, w).shape\n",
    "dy = tf.constant(np.random.randn(*yshape).astype(np.float32))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def tf_conv_backprop_input(x_shape, w, dy):\n",
    "    dx = tf.raw_ops.Conv2DBackpropInput(\n",
    "        input_sizes=x_shape, \n",
    "        filter=w, \n",
    "        out_backprop=dy,\n",
    "        strides=[1, 1, 1, 1], \n",
    "        padding='SAME'\n",
    "    )\n",
    "    return dx\n",
    "\n",
    "\n",
    "fun = tf_conv_backprop_input.get_concrete_function(\n",
    "    list(x.shape),\n",
    "    tf.TensorSpec(w.shape, tf.dtypes.float32),\n",
    "    tf.TensorSpec(dy.shape, tf.dtypes.float32),\n",
    ")\n",
    "mlir = tf.mlir.experimental.convert_function(fun)\n",
    "print(mlir)\n",
    "\n",
    "dx = tf_conv_backprop_input(list(x.shape), w, dy)\n",
    "dx_ref = tf_conv_dx(x, w, dy)\n",
    "print(x.shape, w.shape, dy.shape)\n",
    "print(dx.shape)\n",
    "print(arr_diff(dx, dx_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-craft",
   "metadata": {},
   "source": [
    "Output shape is needed because if stride is > 2, there can be multiple possible output values because of the formula to compute conv2D output\n",
    "\n",
    "```\n",
    "outHeight = (xPHeight - kHeight) // sh + 1\n",
    "```\n",
    "\n",
    "The number is rounded, so it might be more than one possible input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "complicated-grade",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Conv2DCustomBackpropInput: Size of out_backprop doesn't match computed: actual = 29, computed = 30 spatial_dim: 2 input: 30 filter: 5 output: 29 stride: 1 dilation: 1 [Op:Conv2DBackpropInput]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a419ad8171a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mfake_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_conv_backprop_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-a419ad8171a8>\u001b[0m in \u001b[0;36mtf_conv_backprop_input\u001b[0;34m(x_shape, w, dy)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mout_backprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mytf/opgen/_env/lib/python3.6/site-packages/tensorflow/python/util/tf_export.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m           \u001b[0;34m'Please pass these args as kwargs instead.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m           .format(f=f.__name__, kwargs=f_argspec.args))\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mytf/opgen/_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1253\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m           dilations=dilations, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mytf/opgen/_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input_eager_fallback\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1344\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[1;32m   1345\u001b[0m   _result = _execute.execute(b\"Conv2DBackpropInput\", 1, inputs=_inputs_flat,\n\u001b[0;32m-> 1346\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   1347\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/mytf/opgen/_env/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Conv2DCustomBackpropInput: Size of out_backprop doesn't match computed: actual = 29, computed = 30 spatial_dim: 2 input: 30 filter: 5 output: 29 stride: 1 dilation: 1 [Op:Conv2DBackpropInput]"
     ]
    }
   ],
   "source": [
    "strides = [1, 1, 1, 1]\n",
    "padding='SAME'\n",
    "\n",
    "x = tf.constant(np.random.randn(3, 32, 29, 7).astype(np.float32))\n",
    "w = tf.constant(np.random.randn(5, 5, 7, 10).astype(np.float32))\n",
    "yshape = tf.nn.conv2d(input=x, filters=w, strides=strides, padding=padding).shape\n",
    "dy = tf.constant(np.random.randn(*yshape).astype(np.float32))\n",
    "\n",
    "def tf_conv_backprop_input(x_shape, w, dy):\n",
    "    dx = tf.raw_ops.Conv2DBackpropInput(\n",
    "        input_sizes=x_shape, \n",
    "        filter=w, \n",
    "        out_backprop=dy,\n",
    "        strides=strides, \n",
    "        padding=padding\n",
    "    )\n",
    "    return dx\n",
    "\n",
    "fake_shape = [x.shape[0], x.shape[1], x.shape[2], x.shape[3]]\n",
    "\n",
    "# Valid\n",
    "dx = tf_conv_backprop_input(fake_shape, w, dy)\n",
    "\n",
    "# Because stride 1 only one valid shape\n",
    "# Wrong\n",
    "fake_shape[2] = fake_shape[2] + 1\n",
    "dx = tf_conv_backprop_input(fake_shape, w, dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "architectural-karaoke",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Conv2DCustomBackpropInput: Size of out_backprop doesn't match computed: actual = 15, computed = 16 spatial_dim: 2 input: 32 filter: 5 output: 15 stride: 2 dilation: 1 [Op:Conv2DBackpropInput]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-bbc3be7554fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfake_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_conv_backprop_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-bbc3be7554fe>\u001b[0m in \u001b[0;36mtf_conv_backprop_input\u001b[0;34m(x_shape, w, dy)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mout_backprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mytf/opgen/_env/lib/python3.6/site-packages/tensorflow/python/util/tf_export.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m           \u001b[0;34m'Please pass these args as kwargs instead.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m           .format(f=f.__name__, kwargs=f_argspec.args))\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mytf/opgen/_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1253\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m           dilations=dilations, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mytf/opgen/_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input_eager_fallback\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1344\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[1;32m   1345\u001b[0m   _result = _execute.execute(b\"Conv2DBackpropInput\", 1, inputs=_inputs_flat,\n\u001b[0;32m-> 1346\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   1347\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/mytf/opgen/_env/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Conv2DCustomBackpropInput: Size of out_backprop doesn't match computed: actual = 15, computed = 16 spatial_dim: 2 input: 32 filter: 5 output: 15 stride: 2 dilation: 1 [Op:Conv2DBackpropInput]"
     ]
    }
   ],
   "source": [
    "strides = [1, 2, 2, 1]\n",
    "padding='SAME'\n",
    "\n",
    "x = tf.constant(np.random.randn(3, 32, 29, 7).astype(np.float32))\n",
    "w = tf.constant(np.random.randn(5, 5, 7, 10).astype(np.float32))\n",
    "yshape = tf.nn.conv2d(input=x, filters=w, strides=strides, padding=padding).shape\n",
    "dy = tf.constant(np.random.randn(*yshape).astype(np.float32))\n",
    "\n",
    "def tf_conv_backprop_input(x_shape, w, dy):\n",
    "    dx = tf.raw_ops.Conv2DBackpropInput(\n",
    "        input_sizes=x_shape, \n",
    "        filter=w, \n",
    "        out_backprop=dy,\n",
    "        strides=strides, \n",
    "        padding=padding\n",
    "    )\n",
    "    return dx\n",
    "\n",
    "fake_shape = [x.shape[0], x.shape[1], x.shape[2], x.shape[3]]\n",
    "\n",
    "# Valid\n",
    "dx = tf_conv_backprop_input(fake_shape, w, dy)\n",
    "\n",
    "# Valid\n",
    "fake_shape[2] = fake_shape[2] + 1\n",
    "dx = tf_conv_backprop_input(fake_shape, w, dy)\n",
    "\n",
    "# Because stride 2 there can be 2 valid shapes (can add +1), but nore more\n",
    "# Wrong\n",
    "fake_shape[2] = fake_shape[2] + 2\n",
    "dx = tf_conv_backprop_input(fake_shape, w, dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "compressed-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.28893998]\n",
      "   [ 0.6151089 ]\n",
      "   [-0.45987356]\n",
      "   [ 0.97900033]\n",
      "   [-0.12023436]]\n",
      "\n",
      "  [[ 2.409006  ]\n",
      "   [ 0.40683782]\n",
      "   [ 3.8341465 ]\n",
      "   [ 0.6475184 ]\n",
      "   [ 1.0024412 ]]\n",
      "\n",
      "  [[-0.4638604 ]\n",
      "   [ 0.9874877 ]\n",
      "   [-0.57328576]\n",
      "   [ 1.2204375 ]\n",
      "   [-0.17393352]]\n",
      "\n",
      "  [[ 3.8673863 ]\n",
      "   [ 0.653132  ]\n",
      "   [ 4.779709  ]\n",
      "   [ 0.80720687]\n",
      "   [ 1.4501522 ]]\n",
      "\n",
      "  [[ 0.1213702 ]\n",
      "   [-0.25837854]\n",
      "   [ 0.04046171]\n",
      "   [-0.08613677]\n",
      "   [ 0.20346424]]]]\n",
      "\n",
      "======\n",
      "\n",
      "[[[[-0.28893998]\n",
      "   [ 0.6151089 ]\n",
      "   [-0.45987356]\n",
      "   [ 0.97900033]\n",
      "   [-0.12023436]\n",
      "   [ 0.25596052]]\n",
      "\n",
      "  [[ 2.409006  ]\n",
      "   [ 0.40683782]\n",
      "   [ 3.8341465 ]\n",
      "   [ 0.6475184 ]\n",
      "   [ 1.0024412 ]\n",
      "   [ 0.16929428]]\n",
      "\n",
      "  [[-0.4638604 ]\n",
      "   [ 0.9874877 ]\n",
      "   [-0.57328576]\n",
      "   [ 1.2204375 ]\n",
      "   [-0.17393352]\n",
      "   [ 0.3702778 ]]\n",
      "\n",
      "  [[ 3.8673863 ]\n",
      "   [ 0.653132  ]\n",
      "   [ 4.779709  ]\n",
      "   [ 0.80720687]\n",
      "   [ 1.4501522 ]\n",
      "   [ 0.24490462]]\n",
      "\n",
      "  [[ 0.1213702 ]\n",
      "   [-0.25837854]\n",
      "   [ 0.04046171]\n",
      "   [-0.08613677]\n",
      "   [ 0.20346424]\n",
      "   [-0.43314418]]]]\n",
      "\n",
      "======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strides = [1, 2, 2, 1]\n",
    "padding='SAME'\n",
    "\n",
    "x = tf.constant(np.random.randn(1, 5, 5, 1).astype(np.float32))\n",
    "w = tf.constant(np.random.randn(2, 2, 1, 1).astype(np.float32))\n",
    "yshape = tf.nn.conv2d(input=x, filters=w, strides=strides, padding=padding).shape\n",
    "dy = tf.constant(np.random.randn(*yshape).astype(np.float32))\n",
    "\n",
    "def tf_conv_backprop_input(x_shape, w, dy):\n",
    "    dx = tf.raw_ops.Conv2DBackpropInput(\n",
    "        input_sizes=x_shape, \n",
    "        filter=w, \n",
    "        out_backprop=dy,\n",
    "        strides=strides, \n",
    "        padding=padding\n",
    "    )\n",
    "    return dx\n",
    "\n",
    "fake_shape = [x.shape[0], x.shape[1], x.shape[2], x.shape[3]]\n",
    "\n",
    "# Valid\n",
    "dx = tf_conv_backprop_input(fake_shape, w, dy)\n",
    "print(dx.numpy())\n",
    "print('\\n======\\n')\n",
    "\n",
    "# Valid\n",
    "\n",
    "fake_shape[2] = fake_shape[2] + 1\n",
    "dx = tf_conv_backprop_input(fake_shape, w, dy)\n",
    "print(dx.numpy())\n",
    "print('\\n======\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "oriental-detective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Paddings: [[0, 0], [2, 2], [2, 2], [0, 0]]\n",
      "4428097.685704555\n"
     ]
    }
   ],
   "source": [
    "class MyConv2DBackpropInputDescriptor:\n",
    "    \n",
    "    # Compute the input padding, same function than for Conv2D\n",
    "    def get_explicit_padding(self):\n",
    "        padding = self.x_padding\n",
    "        pad_top, pad_bottom, pad_left, pad_right = (None, None, None, None)\n",
    "        xHeight = self.x[1]\n",
    "        xWidth = self.x[2]\n",
    "        kHeight = self.w[0]\n",
    "        kWidth = self.w[1]\n",
    "        sh = self.x_strides[1]\n",
    "        sw = self.x_strides[2]\n",
    "        \n",
    "        if not isinstance(padding, str):\n",
    "            #explicit padding\n",
    "            #[[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]\n",
    "            assert len(padding) == 4\n",
    "            for x in padding: assert len(x) == 2\n",
    "            for x in padding[0]: assert x == 0\n",
    "            for x in padding[3]: assert x == 0\n",
    "            pad_top = padding[1][0]\n",
    "            pad_bottom = padding[1][1]\n",
    "            pad_left = padding[2][0]\n",
    "            pad_right = padding[2][1]\n",
    "        \n",
    "        elif padding == 'SAME':\n",
    "            if xHeight % sh == 0:\n",
    "                pad_height = max(kHeight - sh, 0)\n",
    "            else:\n",
    "                pad_height = max(kHeight - (xHeight % sh), 0)\n",
    "            if xWidth % sw == 0:\n",
    "                pad_width = max(kWidth - sw, 0)\n",
    "            else:\n",
    "                pad_width = max(kWidth - (xWidth % sw), 0)\n",
    "            \n",
    "            pad_top = pad_height // 2\n",
    "            pad_bottom = pad_height - pad_top\n",
    "            pad_left = pad_width // 2\n",
    "            pad_right = pad_width - pad_left\n",
    "            \n",
    "        \n",
    "        elif padding == 'VALID':\n",
    "            pad_top = 0\n",
    "            pad_bottom = 0\n",
    "            pad_left = 0\n",
    "            pad_right = 0\n",
    "            \n",
    "        else:\n",
    "            raise Exception('Unknown padding')\n",
    "        \n",
    "        self.x_padding = [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]\n",
    "            \n",
    "    def get_output_shape(self):\n",
    "        pad_top, pad_bottom = self.padding[1]\n",
    "        pad_left, pad_right = self.padding[2]\n",
    "        xHeight = self.x[1]\n",
    "        xWidth = self.x[2]\n",
    "        kHeight = self.w[0]\n",
    "        kWidth = self.w[1]\n",
    "        sh = self.strides[1]\n",
    "        sw = self.strides[2]\n",
    "        \n",
    "        assert self.x[3] == self.w[2]\n",
    "        \n",
    "        # Padded input dimensions\n",
    "        xPHeight = xHeight + pad_top + pad_bottom\n",
    "        xPWidth = xWidth + pad_left + pad_right\n",
    "        \n",
    "        outN = self.x[0]\n",
    "        outHeight = (xPHeight - kHeight) // sh + 1\n",
    "        outWidth = (xPWidth - kWidth) // sw + 1\n",
    "        outC = self.w[3]\n",
    "        \n",
    "        self.output_shape = [outN, outHeight, outWidth, outC]\n",
    "    \n",
    "    def __init__(self, input_shape, dy, w, strides, padding):\n",
    "        self.x = input_shape\n",
    "        self.w = w\n",
    "        self.y = dy\n",
    "\n",
    "        self.x_strides = strides\n",
    "        self.x_padding = padding\n",
    "                        \n",
    "        # Check strides\n",
    "        assert len(strides) == 4\n",
    "        assert strides[0] == 1\n",
    "        assert strides[3] == 1\n",
    "        \n",
    "        # Compute real paddings\n",
    "        self.get_explicit_padding()\n",
    "        print('Input Paddings:', self.x_padding)\n",
    "        \n",
    "        # Compute output shape\n",
    "        #self.get_output_shape()\n",
    "        #print('Output shape:', self.output_shape)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def compute(self, dy, w):\n",
    "        #TODO\n",
    "        return np.zeros(self.x)\n",
    "        \n",
    "def my_conv2d_backprop_input(input_shape, grad, w, strides, padding):\n",
    "    op = MyConv2DBackpropInputDescriptor(input_shape, grad.shape, w.shape, strides, padding)\n",
    "    return op.compute(grad, w)\n",
    "\n",
    "def ref_conv2d_backprop_input(input_shape, grad, w, strides, padding):\n",
    "    dx = tf.raw_ops.Conv2DBackpropInput(\n",
    "        input_sizes=input_shape, \n",
    "        filter=w, \n",
    "        out_backprop=grad,\n",
    "        strides=strides, \n",
    "        padding=padding\n",
    "    )\n",
    "    return dx\n",
    "\n",
    "strides = [1, 1, 1, 1]\n",
    "padding = 'SAME'\n",
    "\n",
    "x = tf.constant(np.random.randn(3, 32, 29, 7).astype(np.float32))\n",
    "w = tf.constant(np.random.randn(5, 5, 7, 10).astype(np.float32))\n",
    "yshape = tf.nn.conv2d(input=x, filters=w, strides=strides, padding=padding).shape\n",
    "dy = tf.constant(np.random.randn(*yshape).astype(np.float32))\n",
    "input_shape = list(x.shape)\n",
    "\n",
    "dx_ref = ref_conv2d_backprop_input(input_shape, dy, w, strides, padding)\n",
    "dx = my_conv2d_backprop_input(input_shape, dy, w, strides, padding)\n",
    "\n",
    "print(arr_diff(dx_ref, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-webster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
