{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-charge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "def arr_diff(x, y):\n",
    "    assert list(x.shape) == list(y.shape)\n",
    "    x = np.ravel(x)\n",
    "    y = np.ravel(y)\n",
    "    return (x - y) @ (x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-commander",
   "metadata": {},
   "source": [
    "# DepthwiseConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "black-group",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 29, 7) (5, 5, 7, 2) (3, 32, 29, 14)\n",
      "\n",
      "\n",
      "module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 561 : i32}} {\n",
      "  func @__inference_tf_depthwise_conv_22(%arg0: tensor<3x32x29x7xf32> {tf._user_specified_name = \"x\"}, %arg1: tensor<5x5x7x2xf32> {tf._user_specified_name = \"w\"}) -> tensor<3x32x29x14xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"x,w\", outputs = \"identity_RetVal\"}} {\n",
      "    %0 = \"tf.DepthwiseConv2dNative\"(%arg0, %arg1) {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]} : (tensor<3x32x29x7xf32>, tensor<5x5x7x2xf32>) -> tensor<3x32x29x14xf32>\n",
      "    %1 = \"tf.Identity\"(%0) {device = \"\"} : (tensor<3x32x29x14xf32>) -> tensor<3x32x29x14xf32>\n",
      "    return %1 : tensor<3x32x29x14xf32>\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 32, 29, 7)\n",
    "w = np.random.randn(5, 5, 7, 2)\n",
    "\n",
    "@tf.function\n",
    "def tf_depthwise_conv_2d(x, w):\n",
    "    y = tf.nn.depthwise_conv2d(input=x, filter=w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return y\n",
    "\n",
    "y = tf_depthwise_conv(x, w)\n",
    "print(x.shape, w.shape, y.shape)\n",
    "\n",
    "fun = tf_depthwise_conv.get_concrete_function(\n",
    "    tf.TensorSpec(x.shape, tf.dtypes.float32),\n",
    "    tf.TensorSpec(w.shape, tf.dtypes.float32),\n",
    ")\n",
    "mlir = tf.mlir.experimental.convert_function(fun)\n",
    "print(mlir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "optimum-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paddings: [[0, 0], [2, 2], [2, 2], [0, 0]]\n",
      "Output shape: [3, 13, 17, 14]\n",
      "4.7139007055828086e-27\n"
     ]
    }
   ],
   "source": [
    "class MyDepthwiseConv2DDescriptor:\n",
    "    \n",
    "    def get_explicit_padding(self):\n",
    "        padding = self.padding\n",
    "        pad_top, pad_bottom, pad_left, pad_right = (None, None, None, None)\n",
    "        xHeight = self.x[1]\n",
    "        xWidth = self.x[2]\n",
    "        kHeight = self.w[0]\n",
    "        kWidth = self.w[1]\n",
    "        sh = self.strides[1]\n",
    "        sw = self.strides[2]\n",
    "        \n",
    "        if not isinstance(padding, str):\n",
    "            #explicit padding\n",
    "            #[[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]\n",
    "            assert len(padding) == 4\n",
    "            for x in padding: assert len(x) == 2\n",
    "            for x in padding[0]: assert x == 0\n",
    "            for x in padding[3]: assert x == 0\n",
    "            pad_top = padding[1][0]\n",
    "            pad_bottom = padding[1][1]\n",
    "            pad_left = padding[2][0]\n",
    "            pad_right = padding[2][1]\n",
    "        \n",
    "        elif padding == 'SAME':\n",
    "            if xHeight % sh == 0:\n",
    "                pad_height = max(kHeight - sh, 0)\n",
    "            else:\n",
    "                pad_height = max(kHeight - (xHeight % sh), 0)\n",
    "            if xWidth % sw == 0:\n",
    "                pad_width = max(kWidth - sw, 0)\n",
    "            else:\n",
    "                pad_width = max(kWidth - (xWidth % sw), 0)\n",
    "            \n",
    "            pad_top = pad_height // 2\n",
    "            pad_bottom = pad_height - pad_top\n",
    "            pad_left = pad_width // 2\n",
    "            pad_right = pad_width - pad_left\n",
    "            \n",
    "        \n",
    "        elif padding == 'VALID':\n",
    "            pad_top = 0\n",
    "            pad_bottom = 0\n",
    "            pad_left = 0\n",
    "            pad_right = 0\n",
    "            \n",
    "        else:\n",
    "            raise Exception('Unknown padding')\n",
    "        \n",
    "        self.padding = [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]\n",
    "            \n",
    "    def get_output_shape(self):\n",
    "        pad_top, pad_bottom = self.padding[1]\n",
    "        pad_left, pad_right = self.padding[2]\n",
    "        sh, sw = self.strides[1:-1]\n",
    "        xB, xHeight, xWidth, xC = self.x\n",
    "        kHeight, kWidth, _, cMult = self.w\n",
    "        \n",
    "        assert self.x[3] == self.w[2]\n",
    "        \n",
    "        # Padded input dimensions\n",
    "        xPHeight = xHeight + pad_top + pad_bottom\n",
    "        xPWidth = xWidth + pad_left + pad_right\n",
    "        \n",
    "        yHeight = (xPHeight - kHeight) // sh + 1\n",
    "        yWidth = (xPWidth - kWidth) // sw + 1\n",
    "        yC = xC * cMult\n",
    "        \n",
    "        self.output_shape = [xB, yHeight, yWidth, yC]\n",
    "    \n",
    "    def __init__(self, x, w, strides, padding):\n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.output_shape = None\n",
    "                        \n",
    "        # Check strides\n",
    "        assert len(strides) == 4\n",
    "        assert strides[0] == 1\n",
    "        assert strides[3] == 1\n",
    "        \n",
    "        # Compute real paddings\n",
    "        self.get_explicit_padding()\n",
    "        print('Paddings:', self.padding)\n",
    "        \n",
    "        # Compute output shape\n",
    "        self.get_output_shape()\n",
    "        print('Output shape:', self.output_shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # compute using tf version directly\n",
    "    def compute_tf(self, x, w):\n",
    "        y = tf.nn.depthwise_conv2d(input=x, filter=w, strides=self.strides, padding=[[0,0],[0,0],[0,0],[0,0]])\n",
    "        return y.numpy()\n",
    "    \n",
    "    # compute using conv2d operation\n",
    "    def compute_with_conv2d(self, x, w):\n",
    "        sh, sw = self.strides[1:-1]\n",
    "        xB, xHeight, xWidth, xC = self.x\n",
    "        kHeight, kWidth, _, cMult = self.w\n",
    "        yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # we apply a different convolution with chanelMultiplier filters to every chanel of the input\n",
    "        # and we concat on all channels\n",
    "        \n",
    "        res = []\n",
    "        for c in range(xC):\n",
    "            subX = x[:,:,:,c:c+1] # (xB, xH, xW, xC) => (xB, xH, xW, 1)\n",
    "            subW = w[:,:,c:c+1,:] # (kH, kW, xC, cMult) => (kH, kW, 1, cMult)\n",
    "            y = tf.nn.conv2d(input=subX, filters=subW, strides=self.strides, padding=[[0,0],[0,0],[0,0],[0,0]])\n",
    "            res.append(y.numpy())\n",
    "            \n",
    "        return np.concatenate(res, axis=-1)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    # pad input tensor using paddings infos\n",
    "    def pad_input(self, x):\n",
    "        return np.pad(x, self.padding)\n",
    "    \n",
    "    # 7 for loop using depthwise conv2d formula\n",
    "    def compute_naive(self, x, w):\n",
    "        sh, sw = self.strides[1:-1]\n",
    "        xB, xHeight, xWidth, xC = self.x\n",
    "        kHeight, kWidth, _, cMult = self.w\n",
    "        yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        res = np.zeros((xB, yHeight, yWidth, yC))\n",
    "        \n",
    "        for b in range(xB):\n",
    "            for i in range(yHeight):\n",
    "                for j in range(yWidth):\n",
    "                    for k in range(xC):\n",
    "                        for q in range(cMult):\n",
    "                            val = 0\n",
    "                            for di in range(kHeight):\n",
    "                                for dj in range(kWidth):\n",
    "                                        val += x[b, sh*i + di, sw * j + dj, k] * w[di, dj, k, q]\n",
    "                            res[b, i, j, k * cMult + q] = val\n",
    "                                \n",
    "                                \n",
    "                        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "    # flatten and extend the input to run depthwise conv2d as a matmul\n",
    "    # this is the same transformation than for conv2d\n",
    "    # turned from (xB, xH, xW, xC) into (xB*yH*yW, kH*kW*xC)\n",
    "    def matmul_flatten_x(self, x):\n",
    "        sh, sw = self.strides[1:-1]\n",
    "        xB, xHeight, xWidth, xC = self.x\n",
    "        kHeight, kWidth, _, cMult = self.w\n",
    "        yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # divide 1 DIM into 3 to build the mat more easily\n",
    "        new_x = np.zeros((xB, yHeight, yWidth, kHeight*kWidth*xC))\n",
    "        \n",
    "        for b in range(xB):\n",
    "            for i in range(yHeight):\n",
    "                for j in range(yWidth):\n",
    "                    # extract x slice used in the sum of naive implem to compute y[b, i, j, :]\n",
    "                    new_x[b, i, j] = x[b, sh*i:sh*i+kHeight, sw*j:sw*j+kWidth, :].ravel()\n",
    "            \n",
    "        return new_x.reshape(xB * yHeight * yWidth, kHeight * kWidth * xC)\n",
    "    \n",
    "    # flatten and extend the filter to run depthwise conv2d as a matmul\n",
    "    # turned from (kH, kW, xC, cMult) into (kH * kW * xC, xC * cMult)\n",
    "    # It returns a sparse matrix, where in every columns, 2/3 is 0s\n",
    "    # That's because every channel of the input gets multiplied by different filters\n",
    "    # So only the right ones are passed\n",
    "    def matmul_flatten_k(self, w):\n",
    "        sh, sw = self.strides[1:-1]\n",
    "        xB, xHeight, xWidth, xC = self.x\n",
    "        kHeight, kWidth, _, cMult = self.w\n",
    "        yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # divide into many dims to build more easily\n",
    "        new_w = np.zeros((kHeight*kWidth, xC, xC, cMult))\n",
    "        \n",
    "        for k in range(xC):\n",
    "            for q in range(cMult):\n",
    "                new_w[:,k,k,q] = w[:,:,k,q].ravel()\n",
    "                \n",
    "        return new_w.reshape(kHeight*kWidth*xC, xC*cMult)\n",
    "    \n",
    "    # Compute by turning the conv into a matmul op\n",
    "    def compute_matmul(self, x, w):\n",
    "        sh, sw = self.strides[1:-1]\n",
    "        xB, xHeight, xWidth, xC = self.x\n",
    "        kHeight, kWidth, _, cMult = self.w\n",
    "        yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # (xB, xH, xW, xC) -> (xB*yH*yW, kH*kW*xC)\n",
    "        x = self.matmul_flatten_x(x)\n",
    "        \n",
    "        # (kH, kW, xC, cMult) -> (kH * kW * xC, xC * cMult)\n",
    "        w = self.matmul_flatten_k(w)\n",
    "        \n",
    "        # [(xB*yH*yW, kH*kW*xC), (kH * kW * xC, xC * cMult)] -> (xB*yH*yW, xC * cMult)\n",
    "        y = np.matmul(x, w)\n",
    "        \n",
    "        # (xB*yH*yW, xC * cMult) -> (xB, yH, yW, yC)\n",
    "        y = y.reshape(xB, yHeight, yWidth, yC)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def compute(self, x, w):\n",
    "        assert list(x.shape) == list(self.x)\n",
    "        assert list(w.shape) == list(self.w)\n",
    "        \n",
    "        x = self.pad_input(x)\n",
    "        \n",
    "        #y = self.compute_tf(x, w)\n",
    "        #y = self.compute_with_conv2d(x, w)\n",
    "        #y = self.compute_naive(x, w)\n",
    "        y = self.compute_matmul(x, w)\n",
    "        \n",
    "        assert list(y.shape) == list(self.output_shape)\n",
    "        return y\n",
    "        \n",
    "        \n",
    "def my_depthwise_conv2d(x, w, strides, padding):\n",
    "    op = MyDepthwiseConv2DDescriptor(x.shape, w.shape, strides, padding)\n",
    "    return op.compute(x, w)\n",
    "\n",
    "x = np.random.randn(3, 13, 17, 7)\n",
    "w = np.random.randn(5, 5, 7, 2)\n",
    "\n",
    "y_ref = tf_depthwise_conv_2d(x, w).numpy()\n",
    "y = my_depthwise_conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "print(arr_diff(y_ref, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-justice",
   "metadata": {},
   "source": [
    "## DepthwiseConv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "positive-family",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 13, 17, 6, 14)\n",
      "\n",
      "\n",
      "module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 561 : i32}} {\n",
      "  func @__inference_tf_depthwise_conv_3d_1163(%arg0: tensor<3x13x17x6x7xf32> {tf._user_specified_name = \"x\"}, %arg1: tensor<5x5x2x7x2xf32> {tf._user_specified_name = \"w\"}) -> tensor<3x13x17x6x14xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"x,w\", outputs = \"identity_RetVal\"}} {\n",
      "    %0 = \"tf.PartitionedCall\"(%arg0, %arg1) {_collective_manager_ids = [], _read_only_resource_inputs = [], config = \"\", config_proto = \"\\0A\\07\\0A\\03CPU\\10\\01\\0A\\07\\0A\\03GPU\\10\\002\\02J\\008\\01\\82\\01\\00\", device = \"\", executor_type = \"\", f = @__inference_tf_nn_depthwise_conv3d_11600} : (tensor<3x13x17x6x7xf32>, tensor<5x5x2x7x2xf32>) -> tensor<3x13x17x6x14xf32>\n",
      "    %1 = \"tf.Identity\"(%0) {device = \"\"} : (tensor<3x13x17x6x14xf32>) -> tensor<3x13x17x6x14xf32>\n",
      "    return %1 : tensor<3x13x17x6x14xf32>\n",
      "  }\n",
      "  func @__inference_tf_nn_depthwise_conv3d_11600(%arg0: tensor<3x13x17x6x7xf32> {tf._user_specified_name = \"input\"}, %arg1: tensor<5x5x2x7x2xf32> {tf._user_specified_name = \"filter\"}) -> tensor<3x13x17x6x14xf32> attributes {sym_visibility = \"private\"} {\n",
      "    %0 = \"tf.Const\"() {value = dense<-1> : tensor<i32>} : () -> tensor<i32>\n",
      "    %1 = \"tf.Const\"() {value = dense<0> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %2 = \"tf.Const\"() {value = dense<[0, 0, 0, 0, 6]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %3 = \"tf.Const\"() {value = dense<[0, 0, 0, 0, 7]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %4 = \"tf.Const\"() {value = dense<[0, 0, 0, 6, 0]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %5 = \"tf.Const\"() {value = dense<[0, 0, 0, 7, 0]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %6 = \"tf.Const\"() {value = dense<[0, 0, 0, 0, 1]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %7 = \"tf.Const\"() {value = dense<[0, 0, 0, 1, 0]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %8 = \"tf.Const\"() {value = dense<[0, 0, 0, 0, 2]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %9 = \"tf.Const\"() {value = dense<[0, 0, 0, 2, 0]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %10 = \"tf.Const\"() {value = dense<[0, 0, 0, 0, 3]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %11 = \"tf.Const\"() {value = dense<[0, 0, 0, 3, 0]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %12 = \"tf.Const\"() {value = dense<[0, 0, 0, 0, 4]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %13 = \"tf.Const\"() {value = dense<[0, 0, 0, 0, 5]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %14 = \"tf.Const\"() {value = dense<[0, 0, 0, 4, 0]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %15 = \"tf.Const\"() {value = dense<[0, 0, 0, 5, 0]> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %16 = \"tf.Const\"() {value = dense<1> : tensor<5xi32>} : () -> tensor<5xi32>\n",
      "    %17 = \"tf.StridedSlice\"(%arg0, %1, %6, %16) {begin_mask = 15 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 15 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<3x13x17x6x7xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<3x13x17x6x1xf32>\n",
      "    %18 = \"tf.StridedSlice\"(%arg1, %1, %7, %16) {begin_mask = 23 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 23 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<5x5x2x7x2xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<5x5x2x1x2xf32>\n",
      "    %19 = \"tf.Conv3D\"(%17, %18) {data_format = \"NDHWC\", device = \"\", dilations = [1, 1, 1, 1, 1], padding = \"SAME\", strides = [1, 1, 1, 1, 1]} : (tensor<3x13x17x6x1xf32>, tensor<5x5x2x1x2xf32>) -> tensor<3x13x17x6x2xf32>\n",
      "    %20 = \"tf.StridedSlice\"(%arg0, %13, %2, %16) {begin_mask = 15 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 15 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<3x13x17x6x7xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<3x13x17x6x1xf32>\n",
      "    %21 = \"tf.StridedSlice\"(%arg1, %15, %4, %16) {begin_mask = 23 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 23 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<5x5x2x7x2xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<5x5x2x1x2xf32>\n",
      "    %22 = \"tf.Conv3D\"(%20, %21) {data_format = \"NDHWC\", device = \"\", dilations = [1, 1, 1, 1, 1], padding = \"SAME\", strides = [1, 1, 1, 1, 1]} : (tensor<3x13x17x6x1xf32>, tensor<5x5x2x1x2xf32>) -> tensor<3x13x17x6x2xf32>\n",
      "    %23 = \"tf.StridedSlice\"(%arg0, %2, %3, %16) {begin_mask = 15 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 15 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<3x13x17x6x7xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<3x13x17x6x1xf32>\n",
      "    %24 = \"tf.StridedSlice\"(%arg1, %4, %5, %16) {begin_mask = 23 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 23 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<5x5x2x7x2xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<5x5x2x1x2xf32>\n",
      "    %25 = \"tf.Conv3D\"(%23, %24) {data_format = \"NDHWC\", device = \"\", dilations = [1, 1, 1, 1, 1], padding = \"SAME\", strides = [1, 1, 1, 1, 1]} : (tensor<3x13x17x6x1xf32>, tensor<5x5x2x1x2xf32>) -> tensor<3x13x17x6x2xf32>\n",
      "    %26 = \"tf.StridedSlice\"(%arg0, %6, %8, %16) {begin_mask = 15 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 15 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<3x13x17x6x7xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<3x13x17x6x1xf32>\n",
      "    %27 = \"tf.StridedSlice\"(%arg1, %7, %9, %16) {begin_mask = 23 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 23 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<5x5x2x7x2xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<5x5x2x1x2xf32>\n",
      "    %28 = \"tf.Conv3D\"(%26, %27) {data_format = \"NDHWC\", device = \"\", dilations = [1, 1, 1, 1, 1], padding = \"SAME\", strides = [1, 1, 1, 1, 1]} : (tensor<3x13x17x6x1xf32>, tensor<5x5x2x1x2xf32>) -> tensor<3x13x17x6x2xf32>\n",
      "    %29 = \"tf.StridedSlice\"(%arg0, %8, %10, %16) {begin_mask = 15 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 15 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<3x13x17x6x7xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<3x13x17x6x1xf32>\n",
      "    %30 = \"tf.StridedSlice\"(%arg1, %9, %11, %16) {begin_mask = 23 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 23 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<5x5x2x7x2xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<5x5x2x1x2xf32>\n",
      "    %31 = \"tf.Conv3D\"(%29, %30) {data_format = \"NDHWC\", device = \"\", dilations = [1, 1, 1, 1, 1], padding = \"SAME\", strides = [1, 1, 1, 1, 1]} : (tensor<3x13x17x6x1xf32>, tensor<5x5x2x1x2xf32>) -> tensor<3x13x17x6x2xf32>\n",
      "    %32 = \"tf.StridedSlice\"(%arg0, %10, %12, %16) {begin_mask = 15 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 15 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<3x13x17x6x7xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<3x13x17x6x1xf32>\n",
      "    %33 = \"tf.StridedSlice\"(%arg1, %11, %14, %16) {begin_mask = 23 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 23 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<5x5x2x7x2xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<5x5x2x1x2xf32>\n",
      "    %34 = \"tf.Conv3D\"(%32, %33) {data_format = \"NDHWC\", device = \"\", dilations = [1, 1, 1, 1, 1], padding = \"SAME\", strides = [1, 1, 1, 1, 1]} : (tensor<3x13x17x6x1xf32>, tensor<5x5x2x1x2xf32>) -> tensor<3x13x17x6x2xf32>\n",
      "    %35 = \"tf.StridedSlice\"(%arg0, %12, %13, %16) {begin_mask = 15 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 15 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<3x13x17x6x7xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<3x13x17x6x1xf32>\n",
      "    %36 = \"tf.StridedSlice\"(%arg1, %14, %15, %16) {begin_mask = 23 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 23 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64} : (tensor<5x5x2x7x2xf32>, tensor<5xi32>, tensor<5xi32>, tensor<5xi32>) -> tensor<5x5x2x1x2xf32>\n",
      "    %37 = \"tf.Conv3D\"(%35, %36) {data_format = \"NDHWC\", device = \"\", dilations = [1, 1, 1, 1, 1], padding = \"SAME\", strides = [1, 1, 1, 1, 1]} : (tensor<3x13x17x6x1xf32>, tensor<5x5x2x1x2xf32>) -> tensor<3x13x17x6x2xf32>\n",
      "    %38 = \"tf.ConcatV2\"(%19, %28, %31, %34, %37, %22, %25, %0) {device = \"\"} : (tensor<3x13x17x6x2xf32>, tensor<3x13x17x6x2xf32>, tensor<3x13x17x6x2xf32>, tensor<3x13x17x6x2xf32>, tensor<3x13x17x6x2xf32>, tensor<3x13x17x6x2xf32>, tensor<3x13x17x6x2xf32>, tensor<i32>) -> tensor<3x13x17x6x14xf32>\n",
      "    %39 = \"tf.Identity\"(%38) {device = \"\"} : (tensor<3x13x17x6x14xf32>) -> tensor<3x13x17x6x14xf32>\n",
      "    return %39 : tensor<3x13x17x6x14xf32>\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def tf_nn_depthwise_conv3d(input, filter, strides, padding):\n",
    "    res = []\n",
    "    for c in range(input.shape[-1]):\n",
    "        subX = input[:,:,:,:,c:c+1] # (xB, xH, xW, xC) => (xB, xH, xW, 1)\n",
    "        subW = filter[:,:,:,c:c+1,:] # (kH, kW, xC, cMult) => (kH, kW, 1, cMult)\n",
    "        y = tf.nn.conv3d(input=subX, filters=subW, strides=strides, padding=padding)\n",
    "        res.append(y)\n",
    "            \n",
    "    return tf.concat(res, axis=-1)\n",
    "\n",
    "@tf.function\n",
    "def tf_depthwise_conv_3d(x, w):\n",
    "    y = tf_nn_depthwise_conv3d(input=x, filter=w, strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "    return y\n",
    "\n",
    "x = np.random.randn(3, 13, 17, 6, 7)\n",
    "w = np.random.randn(5, 5, 2, 7, 2)\n",
    "\n",
    "y = tf_depthwise_conv_3d(x, w)\n",
    "print(y.shape)\n",
    "\n",
    "fun = tf_depthwise_conv_3d.get_concrete_function(\n",
    "    tf.TensorSpec(x.shape, tf.dtypes.float32),\n",
    "    tf.TensorSpec(w.shape, tf.dtypes.float32),\n",
    ")\n",
    "mlir = tf.mlir.experimental.convert_function(fun)\n",
    "print(mlir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "established-salon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paddings: [[0, 0], [2, 2], [2, 2], [0, 1], [0, 0]]\n",
      "Output shape: [3, 13, 17, 6, 14]\n",
      "1.1480670837803592e-25\n"
     ]
    }
   ],
   "source": [
    "class MyDepthwiseConv3DDescriptor:\n",
    "    \n",
    "    def get_explicit_padding(self):\n",
    "        padding = self.padding\n",
    "        pad_b0, pad_a0, pad_b1, pad_a1, pad_b2, pad_a2 = [None] * 6\n",
    "        sd, sh, sw = self.strides[1:-1]\n",
    "        xB, xDepth, xHeight, xWidth, xC = self.x\n",
    "        kDepth, kHeight, kWidth, _, cMult = self.w\n",
    "        \n",
    "        if not isinstance(padding, str):\n",
    "            #explicit padding\n",
    "            #[[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]\n",
    "            assert len(padding) == 5\n",
    "            for x in padding: assert len(x) == 2\n",
    "            for x in padding[0]: assert x == 0\n",
    "            for x in padding[-1]: assert x == 0\n",
    "            pad_b0 = padding[1][0]\n",
    "            pad_a0 = padding[1][1]\n",
    "            pad_b1 = padding[2][0]\n",
    "            pad_a1 = padding[2][1]\n",
    "            pad_b2 = padding[3][0]\n",
    "            pad_a2 = padding[3][1]\n",
    "        \n",
    "        elif padding == 'SAME':\n",
    "            if xDepth % sd == 0:\n",
    "                pad_depth = max(kDepth - sd, 0)\n",
    "            else:\n",
    "                pad_depth = max(kDepth - (xDepth % sd), 0)\n",
    "            if xHeight % sh == 0:\n",
    "                pad_height = max(kHeight - sh, 0)\n",
    "            else:\n",
    "                pad_height = max(kHeight - (xHeight % sh), 0)\n",
    "            if xWidth % sw == 0:\n",
    "                pad_width = max(kWidth - sw, 0)\n",
    "            else:\n",
    "                pad_width = max(kWidth - (xWidth % sw), 0)\n",
    "            \n",
    "            pad_b0 = pad_depth // 2\n",
    "            pad_a0 = pad_depth - pad_b0\n",
    "            pad_b1 = pad_height // 2\n",
    "            pad_a1 = pad_height - pad_b1\n",
    "            pad_b2 = pad_width // 2\n",
    "            pad_a2 = pad_width - pad_b2\n",
    "            \n",
    "        \n",
    "        elif padding == 'VALID':\n",
    "            pad_b0 = 0\n",
    "            pad_a0 = 0\n",
    "            pad_b1 = 0\n",
    "            pad_a1 = 0\n",
    "            pad_b2 = 0\n",
    "            pad_a2 = 0\n",
    "            \n",
    "        else:\n",
    "            raise Exception('Unknown padding')\n",
    "        \n",
    "        self.padding = [[0, 0], [pad_b0, pad_a0], [pad_b1, pad_a1], [pad_b2, pad_a2], [0, 0]]\n",
    "            \n",
    "    def get_output_shape(self):\n",
    "        pad_b0, pad_a0 = self.padding[1]\n",
    "        pad_top, pad_bottom = self.padding[2]\n",
    "        pad_left, pad_right = self.padding[3]\n",
    "        sd, sh, sw = self.strides[1:-1]\n",
    "        xB, xDepth, xHeight, xWidth, xC = self.x\n",
    "        kDepth, kHeight, kWidth, _, cMult = self.w\n",
    "        \n",
    "        assert self.x[4] == self.w[3]\n",
    "        \n",
    "        # Padded input dimensions\n",
    "        xPDepth = xDepth + pad_b0 + pad_a0\n",
    "        xPHeight = xHeight + pad_top + pad_bottom\n",
    "        xPWidth = xWidth + pad_left + pad_right\n",
    "        \n",
    "        yDepth = (xPDepth - kDepth) // sd + 1\n",
    "        yHeight = (xPHeight - kHeight) // sh + 1\n",
    "        yWidth = (xPWidth - kWidth) // sw + 1\n",
    "        yC = xC * cMult\n",
    "        \n",
    "        self.output_shape = [xB, yDepth, yHeight, yWidth, yC]\n",
    "    \n",
    "    def __init__(self, x, w, strides, padding):\n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.output_shape = None\n",
    "                        \n",
    "        # Check strides\n",
    "        assert len(strides) == 5\n",
    "        assert strides[0] == 1\n",
    "        assert strides[-1] == 1\n",
    "        \n",
    "        # Compute real paddings\n",
    "        self.get_explicit_padding()\n",
    "        print('Paddings:', self.padding)\n",
    "        \n",
    "        # Compute output shape\n",
    "        self.get_output_shape()\n",
    "        print('Output shape:', self.output_shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # compute using tf version directly\n",
    "    def compute_tf(self, x, w):\n",
    "        y = tf_nn_depthwise_conv3d(input=x, filter=w, strides=self.strides, padding='VALID')\n",
    "        return y.numpy()\n",
    "    \n",
    "    # compute using conv3d operation\n",
    "    def compute_with_conv3d(self, x, w):\n",
    "        sd, sh, sw = self.strides[1:-1]\n",
    "        xB, xDepth, xHeight, xWidth, xC = self.x\n",
    "        kDepth, kHeight, kWidth, _, cMult = self.w\n",
    "        yDepth, yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # we apply a different convolution with chanelMultiplier filters to every chanel of the input\n",
    "        # and we concat on all channels\n",
    "        \n",
    "        res = []\n",
    "        for c in range(xC):\n",
    "            subX = x[:,:,:,:,c:c+1] # (xB, xH, xW, xC) => (xB, xH, xW, 1)\n",
    "            subW = w[:,:,:,c:c+1,:] # (kH, kW, xC, cMult) => (kH, kW, 1, cMult)\n",
    "            y = tf.nn.conv3d(input=subX, filters=subW, strides=self.strides, padding='VALID')\n",
    "            res.append(y.numpy())\n",
    "            \n",
    "        return np.concatenate(res, axis=-1)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    # pad input tensor using paddings infos\n",
    "    def pad_input(self, x):\n",
    "        return np.pad(x, self.padding)\n",
    "    \n",
    "    # 9 for loop using depthwise conv3d formula\n",
    "    def compute_naive(self, x, w):\n",
    "        sd, sh, sw = self.strides[1:-1]\n",
    "        xB, xDepth, xHeight, xWidth, xC = self.x\n",
    "        kDepth, kHeight, kWidth, _, cMult = self.w\n",
    "        yDepth, yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        res = np.zeros((xB, yDepth, yHeight, yWidth, yC))\n",
    "        \n",
    "        for b in range(xB):\n",
    "            for h in range(yDepth):\n",
    "                for i in range(yHeight):\n",
    "                    for j in range(yWidth):\n",
    "                        for k in range(xC):\n",
    "                            for q in range(cMult):\n",
    "                                val = 0\n",
    "                                for dh in range(kDepth):\n",
    "                                    for di in range(kHeight):\n",
    "                                        for dj in range(kWidth):\n",
    "                                            val += x[b, sd*h + dh, sh*i + di, sw * j + dj, k] * w[dh, di, dj, k, q]\n",
    "                                res[b, h, i, j, k * cMult + q] = val\n",
    "                                \n",
    "                                \n",
    "                        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "    # flatten and extend the input to run depthwise conv3d as a matmul\n",
    "    # this is the same transformation than for conv3d\n",
    "    # turned from (xB, xD, xH, xW, xC) into (xB*yD*yH*yW, kD*kH*kW*xC)\n",
    "    def matmul_flatten_x(self, x):\n",
    "        sd, sh, sw = self.strides[1:-1]\n",
    "        xB, xDepth, xHeight, xWidth, xC = self.x\n",
    "        kDepth, kHeight, kWidth, _, cMult = self.w\n",
    "        yDepth, yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # divide 1 DIM into 4 to build the mat more easily\n",
    "        new_x = np.zeros((xB, yDepth, yHeight, yWidth, kDepth*kHeight*kWidth*xC))\n",
    "        \n",
    "        for b in range(xB):\n",
    "            for h in range(yDepth):\n",
    "                for i in range(yHeight):\n",
    "                    for j in range(yWidth):\n",
    "                        # extract x slice used in the sum of naive implem to compute y[b, i, j, :]\n",
    "                        new_x[b, h, i, j] = x[b, sd*h:sd*h+kDepth, sh*i:sh*i+kHeight, sw*j:sw*j+kWidth, :].ravel()\n",
    "            \n",
    "        return new_x.reshape(xB * yDepth * yHeight * yWidth, kDepth * kHeight * kWidth * xC)\n",
    "    \n",
    "    # flatten and extend the filter to run depthwise conv3d as a matmul\n",
    "    # turned from (kD, kH, kW, xC, cMult) into (kD * kH * kW * xC, xC * cMult)\n",
    "    # It returns a sparse matrix, where in every columns, 2/3 is 0s\n",
    "    # That's because every channel of the input gets multiplied by different filters\n",
    "    # So only the right ones are passed\n",
    "    def matmul_flatten_k(self, w):\n",
    "        sd, sh, sw = self.strides[1:-1]\n",
    "        xB, xDepth, xHeight, xWidth, xC = self.x\n",
    "        kDepth, kHeight, kWidth, _, cMult = self.w\n",
    "        yDepth, yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # divide into many dims to build more easily\n",
    "        new_w = np.zeros((kDepth*kHeight*kWidth, xC, xC, cMult))\n",
    "        \n",
    "        for k in range(xC):\n",
    "            for q in range(cMult):\n",
    "                new_w[:,k,k,q] = w[:,:,:,k,q].ravel()\n",
    "                \n",
    "        return new_w.reshape(kDepth*kHeight*kWidth*xC, xC*cMult)\n",
    "    \n",
    "    # Compute by turning the conv into a matmul op\n",
    "    def compute_matmul(self, x, w):\n",
    "        sd, sh, sw = self.strides[1:-1]\n",
    "        xB, xDepth, xHeight, xWidth, xC = self.x\n",
    "        kDepth, kHeight, kWidth, _, cMult = self.w\n",
    "        yDepth, yHeight, yWidth, yC  = self.output_shape[1:]\n",
    "        \n",
    "        # (xB, xD, xH, xW, xC) -> (xB*yD*yH*yW, kD*kH*kW*xC)\n",
    "        x = self.matmul_flatten_x(x)\n",
    "        \n",
    "        # (kD, kH, kW, xC, cMult) -> (kD * kH * kW * xC, xC * cMult)\n",
    "        w = self.matmul_flatten_k(w)\n",
    "        \n",
    "        # [(xB*yD*yH*yW, kD*kH*kW*xC), (kD * kH * kW * xC, xC * cMult)] -> (xB*yD*yH*yW, xC * cMult)\n",
    "        y = np.matmul(x, w)\n",
    "        \n",
    "        # (xB*yD*yH*yW, xC * cMult) -> (xB, yD, yH, yW, yC)\n",
    "        y = y.reshape(xB, yDepth, yHeight, yWidth, yC)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def compute(self, x, w):\n",
    "        assert list(x.shape) == list(self.x)\n",
    "        assert list(w.shape) == list(self.w)\n",
    "        \n",
    "        x = self.pad_input(x)\n",
    "        \n",
    "        #y = self.compute_tf(x, w)\n",
    "        #y = self.compute_with_conv3d(x, w)\n",
    "        #y = self.compute_naive(x, w)\n",
    "        y = self.compute_matmul(x, w)\n",
    "        \n",
    "        assert list(y.shape) == list(self.output_shape)\n",
    "        return y\n",
    "        \n",
    "        \n",
    "def my_depthwise_conv3d(x, w, strides, padding):\n",
    "    op = MyDepthwiseConv3DDescriptor(x.shape, w.shape, strides, padding)\n",
    "    return op.compute(x, w)\n",
    "\n",
    "x = np.random.randn(3, 13, 17, 6, 7)\n",
    "w = np.random.randn(5, 5, 2, 7, 2)\n",
    "\n",
    "y_ref = tf_depthwise_conv_3d(x, w).numpy()\n",
    "y = my_depthwise_conv3d(x, w, strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "print(arr_diff(y_ref, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
